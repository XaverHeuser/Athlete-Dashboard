{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34384c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "_GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID')\n",
    "_BQ_DATASET_MARTS = os.getenv('BIGQUERY_DATASET_MARTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    creds = service_account.Credentials.from_service_account_info(\n",
    "        st.secrets['gcp_service_account']\n",
    "    )\n",
    "    client = bigquery.Client(credentials=creds, project=creds.project_id)\n",
    "except Exception:\n",
    "    # Local dev fallback (requires GOOGLE_APPLICATION_CREDENTIALS)\n",
    "    client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57820b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activities\n",
    "query_activities = f\"SELECT * FROM {_GCP_PROJECT_ID}.{_BQ_DATASET_MARTS}.fct_activities\"\n",
    "df_activities = client.query(query_activities).to_dataframe()\n",
    "\n",
    "print(f'Count activities: {len(df_activities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = df_activities[df_activities['discipline'] == 'Run'].copy()\n",
    "print(f'Len relevant activities: {len(df_runs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73386580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_runs.copy()\n",
    "\n",
    "# Required columns check\n",
    "# TODO: Add more cols\n",
    "req = [\"start_date_local\", \"distance_km\", \"moving_time_s\"]\n",
    "missing = [c for c in req if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "df[\"start_date_local\"] = pd.to_datetime(df[\"start_date_local\"])\n",
    "df = df.sort_values(\"start_date_local\").reset_index(drop=True)\n",
    "\n",
    "# Compute pace_sec_per_km if missing\n",
    "if \"pace_sec_per_km\" not in df.columns:\n",
    "    df[\"pace_sec_per_km\"] = df[\"moving_time_s\"] / df[\"distance_km\"]\n",
    "\n",
    "# Basic sanity filtering (tune)\n",
    "df = df[(df[\"distance_km\"] > 0) & (df[\"moving_time_s\"] > 0)].copy()\n",
    "df = df[(df[\"pace_sec_per_km\"] >= 120) & (df[\"pace_sec_per_km\"] <= 900)].copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df[\"start_date_local\"].dt.date\n",
    "\n",
    "# Time-weighted pace: total_time / total_distance\n",
    "daily = (\n",
    "    df.groupby(\"date\", as_index=False)\n",
    "      .agg(\n",
    "          runs=(\"distance_km\", \"size\"),\n",
    "          distance_km=(\"distance_km\", \"sum\"),\n",
    "          time_s=(\"moving_time_s\", \"sum\"),\n",
    "          elev_gain_m=(\"elevation_gain_m\", \"sum\") if \"elevation_gain_m\" in df.columns else (\"moving_time_s\", \"sum\")\n",
    "          )\n",
    ")\n",
    "\n",
    "# Get weighted average pace\n",
    "daily[\"pace_sec_per_km\"] = daily[\"time_s\"] / daily[\"distance_km\"]\n",
    "\n",
    "# HR time-weighted mean (only if available)\n",
    "if \"avg_heartrate\" in df.columns:\n",
    "    if \"has_heartrate\" in df.columns:\n",
    "        dhr = df[df[\"has_heartrate\"].fillna(False)].copy()\n",
    "    else:\n",
    "        dhr = df[df[\"avg_heartrate\"].notna()].copy()\n",
    "\n",
    "    if len(dhr) > 0:\n",
    "        dhr[\"date\"] = dhr[\"start_date_local\"].dt.date\n",
    "        hr_daily = (\n",
    "            dhr.groupby(\"date\", as_index=False)\n",
    "               .apply(lambda g: pd.Series({\n",
    "                   \"hr_mean\": np.sum(g[\"avg_heartrate\"] * g[\"moving_time_s\"]) / np.sum(g[\"moving_time_s\"])\n",
    "               }))\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "        daily = daily.merge(hr_daily, on=\"date\", how=\"left\")\n",
    "    else:\n",
    "        daily[\"hr_mean\"] = np.nan\n",
    "else:\n",
    "    daily[\"hr_mean\"] = np.nan\n",
    "\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7857eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing dates\n",
    "daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "daily = daily.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "full_dates = pd.date_range(daily[\"date\"].min(), daily[\"date\"].max(), freq=\"D\")\n",
    "daily = daily.set_index(\"date\").reindex(full_dates).rename_axis(\"date\").reset_index()\n",
    "\n",
    "# Fill \"no training day\" with zeros where appropriate\n",
    "daily[\"runs\"] = daily[\"runs\"].fillna(0).astype(int)\n",
    "daily[\"distance_km\"] = daily[\"distance_km\"].fillna(0.0)\n",
    "daily[\"time_s\"] = daily[\"time_s\"].fillna(0.0)\n",
    "daily[\"elev_gain_m\"] = daily[\"elev_gain_m\"].fillna(0.0)\n",
    "\n",
    "# pace & hr are undefined on rest days\n",
    "daily.loc[daily[\"distance_km\"] == 0, \"pace_sec_per_km\"] = np.nan\n",
    "daily.loc[daily[\"distance_km\"] == 0, \"hr_mean\"] = np.nan\n",
    "\n",
    "daily.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a personal reference pace (baseline intensity)\n",
    "\n",
    "# Why median?\n",
    "# - Robust to outliers (races, very easy days)\n",
    "# - Represents a \"typical aerobic running pace\"\n",
    "# - Stable over long periods\n",
    "base_pace = daily[\"pace_sec_per_km\"].median(skipna=True)  # sec/km\n",
    "\n",
    "\n",
    "# Compute relative intensity for each day\n",
    "\n",
    "# intensity = base_pace / actual_pace\n",
    "# Interpretation:\n",
    "# - intensity = 1.0 - running at baseline pace\n",
    "# - intensity > 1.0 - faster than baseline (harder day)\n",
    "# - intensity < 1.0 - slower than baseline (easier day)\n",
    "daily[\"intensity\"] = np.where(\n",
    "    daily[\"pace_sec_per_km\"].notna(),\n",
    "    base_pace / daily[\"pace_sec_per_km\"],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "\n",
    "# Compute daily training load (\"effective hours\")\n",
    "\n",
    "# Training load combines:\n",
    "# - DURATION (how long you trained)\n",
    "# - INTENSITY (how hard you trained relative to baseline) squared because of hard impact\n",
    "# load = (time in hours) × (relative intensity**2)\n",
    "P_INTENSITY = 2\n",
    "\n",
    "daily[\"load\"] = (daily[\"time_s\"] / 3600.0) * (daily[\"intensity\"] **P_INTENSITY)  # \"effective hours\"\n",
    "daily.sort_values(by='load').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_load(load: pd.Series, tau_days: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute an Exponentially Weighted Moving Average (EWMA)\n",
    "    with a specified decay time constant (tau_days).\n",
    "\n",
    "    alpha determines how much weight is given to \"today\"\n",
    "    versus the accumulated past.\n",
    "\n",
    "    This formulation makes tau directly interpretable:\n",
    "    - After tau days, the influence of a single training impulse\n",
    "      has decayed to ~37% of its original effect.\n",
    "\n",
    "    EWMA turns training history into a living memory of your body.\n",
    "    It remembers:\n",
    "      how hard you trained\n",
    "      how recently\n",
    "      and forgets at the right speed\n",
    "    \"\"\"\n",
    "    # EWMA with decay constant tau\n",
    "    alpha = 1 - np.exp(-1 / tau_days)\n",
    "    return load.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# Define time constants (sport science defaults)\n",
    "# Fitness builds and decays slowly → long time constant\n",
    "# Fatigue builds and decays quickly → short time constant\n",
    "TAU_FITNESS = 42\n",
    "TAU_FATIGUE = 7\n",
    "\n",
    "# Compute latent training states\n",
    "daily[\"fitness\"] = ewma_load(daily[\"load\"], TAU_FITNESS)\n",
    "daily[\"fatigue\"] = ewma_load(daily[\"load\"], TAU_FATIGUE)\n",
    "\n",
    "# Readiness = how much fitness is \"available\" after fatigue\n",
    "daily[\"readiness\"] = daily[\"fitness\"] - daily[\"fatigue\"]\n",
    "\n",
    "daily.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95238910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily load\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily[\"date\"], daily[\"load\"])\n",
    "plt.title(\"Daily Training Load\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Load\")\n",
    "plt.show()\n",
    "\n",
    "# Plot fitness, fatigue, readiness\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(daily[\"date\"], daily[\"fitness\"], label=\"fitness\")\n",
    "plt.plot(daily[\"date\"], daily[\"fatigue\"], label=\"fatigue\")\n",
    "plt.plot(daily[\"date\"], daily[\"readiness\"], label=\"readiness\")\n",
    "plt.title(\"Fitness / Fatigue / Readiness (EWMA)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
